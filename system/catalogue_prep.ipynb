{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "path = 'C:/Users/2093/Desktop/Data Center/03. Data/05. TAITRA/TT/'\n",
    "\n",
    "codes = pd.read_csv(path + 'ebs_view_CodeSets_20170522.csv',\n",
    "                    usecols=['new_CodeSetsId', 'new_CodeType', 'new_CodeValue', 'new_CodeValueLength',\n",
    "                             'new_NameCht', 'new_NameEng'])\n",
    "codes.columns = ['code_id', 'code_type', 'code_val', 'code_len', 'ch_name', 'en_name']\n",
    "code_prod_map = pd.read_csv(path + 'ebs_view_new_CodeSets_new_Products_20170522.csv',\n",
    "                            usecols=['new_codesetsid', 'new_productsid'])\n",
    "code_prod_map.columns = ['code_id', 'prod_id']\n",
    "ctlg = pd.read_csv(path + 'ebs_view_product_20170522.csv',\n",
    "                   usecols=['new_BAN', 'new_ProductsId', 'new_name', 'new_DescTextEng',\n",
    "                             'new_KeywordEng', 'new_modified', 'new_TTImage'],\n",
    "                   dtype={'new_BAN': str},\n",
    "                   parse_dates=['new_modified'])\n",
    "ctlg.columns = ['ban', 'prod_id', 'prod_name', 'prod_desc', 'keyword', 'mod_date', 'image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111 items have two TAITRA codes attached.\n"
     ]
    }
   ],
   "source": [
    "print('{} items have two TAITRA codes attached.'\n",
    "      .format((code_prod_map['prod_id'].value_counts() >= 2).sum()))\n",
    "\n",
    "code_prod_map.drop_duplicates(['prod_id'], inplace=True)\n",
    "assert (code_prod_map['prod_id'].value_counts() >= 2).sum() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Join three DataFrames\n",
    "ctlg = (ctlg.merge(code_prod_map, how='left', on='prod_id').merge(codes, how='left', on='code_id')\n",
    "        .drop(['prod_id', 'code_id', 'code_type', 'code_len', 'ch_name', 'en_name'], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9,924 items do not have TAITRA code attached.\n",
      "These belong to 2,450 unique suppliers, of which 665 do not have any other items with TAITRA code (1.87% of all suppliers).\n"
     ]
    }
   ],
   "source": [
    "no_code = ctlg['code_val'].isnull()\n",
    "\n",
    "print(('{:,} items do not have TAITRA code attached.\\nThese belong to {:,} unique suppliers, \\\n",
    "of which {} do not have any other items with TAITRA code ({}% of all suppliers).'\n",
    "       .format(no_code.sum(),\n",
    "               ctlg[no_code]['ban'].nunique(),\n",
    "               len(set(ctlg[no_code]['ban'].unique()) - set(ctlg[~no_code]['ban'].unique())),\n",
    "               round(len(set(ctlg[no_code]['ban'].unique()) - set(ctlg[~no_code]['ban'].unique()))\n",
    "                     / ctlg['ban'].nunique() * 100, 2))))\n",
    "\n",
    "ctlg = ctlg[ctlg['code_val'].notnull()]\n",
    "assert ctlg['code_val'].isnull().sum() == 0\n",
    "ctlg['code_val'] = ctlg['code_val'].astype(int).astype(str).str.zfill(6).astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of non-missing values before removal:\n",
      "ban          0.999625\n",
      "prod_name    0.599947\n",
      "prod_desc    0.586136\n",
      "keyword      0.368742\n",
      "mod_date     0.997704\n",
      "image        0.849785\n",
      "code_val     1.000000\n",
      "dtype: float64\n",
      "\n",
      "If we remove all items without product name, 1,249 suppliers will be lost (3.58% of total number of suppliers).\n",
      "\n",
      "Fraction of non-missing values after removal:\n",
      "ban          0.999380\n",
      "prod_name    1.000000\n",
      "prod_desc    0.976939\n",
      "keyword      0.401397\n",
      "mod_date     0.996252\n",
      "image        0.780910\n",
      "code_val     1.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print('Fraction of non-missing values before removal:')\n",
    "print(ctlg.notnull().sum() / len(ctlg))\n",
    "\n",
    "n_supp_lost = ctlg['ban'].nunique() - ctlg[ctlg['prod_name'].notnull()]['ban'].nunique()\n",
    "\n",
    "print('\\nIf we remove all items without product name, {:,} suppliers will be lost ({}% of total number \\\n",
    "of suppliers).\\n'.format(n_supp_lost, round(n_supp_lost / ctlg['ban'].nunique() * 100, 2)))\n",
    "\n",
    "ctlg = ctlg[ctlg['prod_name'].notnull()]\n",
    "assert ctlg['prod_name'].isnull().sum() == 0\n",
    "\n",
    "print('Fraction of non-missing values after removal:')\n",
    "print(ctlg.notnull().sum() / len(ctlg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "sno = SnowballStemmer('english')\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def process_string(s):\n",
    "    processed = (s.str.strip()\n",
    "                  .str.lower()\n",
    "                  .str.replace(r'[\\t\\n\\r\\f\\v]', r'')\n",
    "                  .str.replace(r'\\d+', r'')\n",
    "                  # capture commas followed by any number of whitespaces\n",
    "                  .str.replace(r', *', r' ')\n",
    "                  .apply(lambda s: s.translate(str.maketrans({x: None for x in string.punctuation}))\n",
    "                         if type(s) == str else '')\n",
    "                  # apply SnowballStemmer then WordNetLemmatizer to singularize missed words\n",
    "                  .apply(lambda s: ' '.join(set([wnl.lemmatize(sno.stem(x)) for x in re.split(r' +', s)\n",
    "                                                 if x not in stopwords.words('english')]))\n",
    "                         if type(s) == str else ''))\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1h 22min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "ctlg['prod_name'] = process_string(ctlg['prod_name'])\n",
    "ctlg['prod_desc'] = process_string(ctlg['prod_desc'])\n",
    "ctlg['keyword'] = process_string(ctlg['keyword'])\n",
    "\n",
    "ctlg.to_csv(path + 'processed_ctlg.csv', index=False, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
